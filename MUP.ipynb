{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pyhessian import hessian\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import scipy as sp\n",
    "\n",
    "epochs = 200\n",
    "n_tasks = 1\n",
    "L = 0\n",
    "gamma0 = 1\n",
    "widths = [128]\n",
    "device = 'mps'\n",
    "gen = torch.Generator(device=device)\n",
    "gen.manual_seed(123)\n",
    "batch = 20\n",
    "\n",
    "class MLP(nn.Module):\n",
    "            def __init__(self, w, L, param, gam):\n",
    "                super(MLP, self).__init__()\n",
    "                self.w = w\n",
    "                if param =='ntk':\n",
    "                    self.gamma = gam\n",
    "                    self.in_scale = 784**0.5\n",
    "                    self.out_scale = self.w**0.5*self.gamma\n",
    "                elif param == 'mup': \n",
    "                     self.gamma = gam*self.w**0.5\n",
    "                     self.in_scale = 784**0.5\n",
    "                     self.out_scale = self.w**0.5*self.gamma\n",
    "                elif param == 'sp':\n",
    "                     self.gamma = 1\n",
    "                     self.in_scale = 1\n",
    "                     self.out_scale = 1\n",
    "\n",
    "                self.fc1 = nn.Linear(784, self.w, bias=False)\n",
    "                self.fc2 = nn.Linear(self.w, 10, bias=False)\n",
    "                self.relu = nn.ReLU()\n",
    "                self.L = L\n",
    "\n",
    "            def forward(self, x):\n",
    "                h1 = self.fc1(x)/self.in_scale\n",
    "                h1act = self.relu(h1)\n",
    "                h2 = self.fc2(h1act)/self.out_scale\n",
    "\n",
    "                return h2\n",
    "            \n",
    "@torch.no_grad()\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.normal_()\n",
    "\n",
    "def permut_row(x, perm):\n",
    "            return x[perm]\n",
    "\n",
    "        # ---------------------- START DATA -------------------------\n",
    "data = pd.read_csv('~/data/MNIST/mnist_train.csv')\n",
    "test = pd.read_csv('~/data/MNIST/mnist_test.csv')\n",
    "#data = data[data['label'].isin([0, 1])]\n",
    "#test = test[test['label'].isin([0, 1])]\n",
    "X = torch.tensor(data.drop('label', axis = 1).to_numpy(), device=device)/255\n",
    "X_test = torch.tensor(test.drop('label', axis = 1).to_numpy(), device=device)/255\n",
    "X = X[:20]\n",
    "\n",
    "Y_temp = torch.tensor(data['label'].to_numpy(), device=device)\n",
    "Y = torch.eye(10, device=device)[Y_temp]\n",
    "Y = Y[:20]\n",
    "\n",
    "Y_temp = torch.tensor(test['label'].to_numpy(), device=device)\n",
    "Y_test = torch.eye(10, device=device)[Y_temp]\n",
    "\n",
    "tasks = [X]\n",
    "tasks_test = [X_test]\n",
    "\n",
    "for _ in range(n_tasks):\n",
    "        perm = np.random.permutation(X.shape[1])\n",
    "        tasks.append( torch.tensor(np.apply_along_axis(permut_row, axis = 1, arr=X.cpu(), perm=perm)).to(device) )\n",
    "        tasks_test.append(torch.tensor(np.apply_along_axis(permut_row, axis = 1, arr=X_test.cpu(), perm=perm)).to(device))\n",
    "\n",
    "\n",
    "def top_eigen(model, loss, X, Y, prt=False):\n",
    "\n",
    "            hess_comp = hessian(model, loss, (X,Y) )\n",
    "            top_eigenvalues, top_eigenvector = hess_comp.eigenvalues()\n",
    "                \n",
    "            return top_eigenvalues[-1] , top_eigenvector\n",
    "\n",
    "def overlap(model, inputs, targets):\n",
    "    \n",
    "    gradients = torch.cat([param.grad.view(-1) for param in model.parameters()])\n",
    "    params = torch.cat([param.data.view(-1) for param in model.parameters()])\n",
    "\n",
    "    norm = torch.norm(params)\n",
    "\n",
    "    def loss_fn(params):\n",
    "        idx = 0\n",
    "        layers = []\n",
    "        for param in model.parameters():\n",
    "            param_numel = param.numel()\n",
    "            layers.append(params[idx:idx + param_numel].view_as(param))\n",
    "            idx += param_numel\n",
    "        relu = nn.ReLU()\n",
    "        outputs = relu(inputs@layers[0].T) @ layers[1].T\n",
    "        return MSE(outputs, targets)\n",
    "\n",
    "    hvp = torch.autograd.functional.hvp(loss_fn, params, gradients)[1]\n",
    "    return (torch.dot(hvp,gradients)/(torch.norm(hvp) * torch.norm(gradients))).item(), norm.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 128]         100,352\n",
      "              ReLU-2               [-1, 1, 128]               0\n",
      "            Linear-3                [-1, 1, 10]           1,280\n",
      "================================================================\n",
      "Total params: 101,632\n",
      "Trainable params: 101,632\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.39\n",
      "Estimated Total Size (MB): 0.39\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/autograd/graph.py:690: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/engine.cpp:1183.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training task0, train loss: 0.0043152239173650745\n",
      "Finished Training task1, train loss: 0.003935420885682106\n"
     ]
    }
   ],
   "source": [
    "save_out = False\n",
    "\n",
    "for regime in ['ntk']:\n",
    "    for N in widths:\n",
    "\n",
    "        loss_hist = []\n",
    "        lam = []\n",
    "        acc = []\n",
    "        all = []\n",
    "        norm = []\n",
    "        res1 = []\n",
    "        res2 = []\n",
    "        \n",
    "        mlp = MLP(N,L,regime, gamma0)\n",
    "\n",
    "        if regime == 'ntk' or regime == 'mup':\n",
    "            mlp = mlp.apply(init_weights)\n",
    "            \n",
    "        summary(mlp, (1,784))\n",
    "        mlp = mlp.to(device)\n",
    "        \n",
    "        optimizer = torch.optim.SGD(mlp.parameters(), lr= mlp.gamma**2)\n",
    "        eos = 2/mlp.gamma**2\n",
    "     \n",
    "        MSE = nn.MSELoss()\n",
    "\n",
    "        for t,Xt in enumerate(tasks):        \n",
    "                for epoch in range(epochs):\n",
    "\n",
    "                        running_loss = 0.0\n",
    "                        for i in range(len(Xt)//batch):\n",
    "\n",
    "                            # Batch of training \n",
    "                            ix = torch.randint(0, len(X), (batch,), generator=gen, device=device)\n",
    "\n",
    "                            ixc = torch.randint(0, len(X), (1024,), generator=gen, device=device)\n",
    "\n",
    "                            lt = []\n",
    "                            for s in range(t+1):\n",
    "                                sharp, eigen = top_eigen(mlp, MSE, tasks[s][ixc], Y[ixc])\n",
    "                                lt.append(sharp)\n",
    "                            lam.append(lt)    \n",
    "\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                            out = mlp(Xt[ix])\n",
    "                            loss = MSE(out, Y[ix])\n",
    "\n",
    "                            #res1.append( list((torch.sum(mlp(tasks[0][ix]) - Y[ix], dim=1)).detach().cpu()) )\n",
    "                            #res2.append( list((torch.sum(mlp(tasks[1][ix]) - Y[ix], dim=1)).detach().cpu()) )\n",
    "                            \n",
    "                            loss.backward()\n",
    "                            running_loss += loss.item()\n",
    "\n",
    "                            over, no = overlap(mlp, X[ix], Y[ix])\n",
    "\n",
    "                            all.append(over)\n",
    "                            norm.append(no)\n",
    "                            \n",
    "                            optimizer.step()\n",
    "                            loss_hist.append(loss.item())\n",
    "\n",
    "                            #print(f'task {t} : (epoch: {epoch}), sample: {batch*(i+1)}, ---> train loss = {loss.item():.4f}')\n",
    "\n",
    "                print(f'Finished Training task{t}, train loss: {running_loss/batch}')\n",
    "                \n",
    "                acct = []\n",
    "                for s in range(t+1):\n",
    "                    acct.append( (torch.sum(torch.argmax(mlp(tasks_test[s]), dim=1) == torch.argmax(Y_test, dim=1))/len(Y_test)).item() )  \n",
    "                acc.append(acct) \n",
    "                \n",
    "        if save_out:        \n",
    "            with open(f'/Users/alessandrobreccia/Desktop/THESIS/data/lamda{N}_{regime}.txt', 'w') as file:\n",
    "            \n",
    "                for lst in lam:\n",
    "                    file.write(' '.join(map(str, lst)) + ' ')\n",
    "        \n",
    "            with open(f'/Users/alessandrobreccia/Desktop/THESIS/data/acc{N}_{regime}.txt', 'w') as file:\n",
    "\n",
    "                for lst in acc:\n",
    "                    file.write(' '.join(map(str, lst)) + ' ')\n",
    "        \n",
    "            with open(f'/Users/alessandrobreccia/Desktop/THESIS/data/overlap{N}_{regime}.txt', 'w') as file:\n",
    "\n",
    "                for a in all:\n",
    "                    file.write(str(a) + ' ')\n",
    "        \n",
    "            with open(f'/Users/alessandrobreccia/Desktop/THESIS/data/norm{N}_{regime}.txt', 'w') as file:\n",
    "\n",
    "                for n in norm:\n",
    "                    file.write(str(n) + ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SHARP():\n",
    "    T = epochs*len(X)//batch\n",
    "    colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'gray', 'orange', 'purple']\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.title(f'Sharpness evolution with net of L={L}, lr={2/eos}, batch={batch}')\n",
    "    N = 128\n",
    "    for regime,ls in zip(['sp','ntk','mup'],['-',':']):\n",
    "\n",
    "        lam_rec = []\n",
    "\n",
    "        with open(f'/Users/alessandrobreccia/Desktop/THESIS/data/lamda{N}_{regime}.txt', 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            lst = list(map(float, line.strip(',').split()))\n",
    "            lam_rec.append(lst)\n",
    "                \n",
    "        a = np.array(lam_rec).T\n",
    "\n",
    "        sh1 = a[0:T]\n",
    "        sh2 = a[T:3*T]\n",
    "        sh3 = a[3*T:6*T]\n",
    "        sh4 = a[6*T:10*T]\n",
    "    \n",
    "        for i,sh in enumerate([sh1,sh2,sh3,sh4]):\n",
    "            plt.axvline(i*T, color='r', linestyle='dotted')\n",
    "            for j,row in enumerate(sh.reshape(T,i+1).T):\n",
    "                plt.plot(np.arange(i*T,(i+1)*T,1), row, color=colors[j], linestyle=ls)\n",
    "\n",
    "    plt.ylabel('Sharpness')\n",
    "    plt.xlabel('Epochs')\n",
    "    #plt.axhline(eos, color='black', linestyle='dotted')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "SHARP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACC():\n",
    "    T = epochs*len(X)//batch\n",
    "    colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'gray', 'orange', 'purple']\n",
    "    N = 128\n",
    "    for regime,ls in zip(['sp','ntk','mup'],['o','*','+']):\n",
    "\n",
    "        acc_rec = []\n",
    "\n",
    "        with open(f'/Users/alessandrobreccia/Desktop/THESIS/data/acc{N}_{regime}.txt', 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            lst = list(map(float, line.strip(',').split()))\n",
    "            acc_rec.append(lst)\n",
    "                \n",
    "        a = np.array(acc_rec).T\n",
    "\n",
    "        accs = [a[:1],a[1:3],a[3:6],a[6:10]]\n",
    "\n",
    "        for i in range(n_tasks):\n",
    "                plt.plot(range(i,n_tasks+1),[a[i] for a in accs[i:]], color=colors[i], marker=ls)\n",
    "        plt.plot(n_tasks,accs[-1][-1], color=colors[-1], marker=ls )\n",
    "        \n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Tasks')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "ACC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OVERLAP():\n",
    "    T = epochs*len(X)//batch\n",
    "    colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'gray', 'orange', 'purple']\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.title(f'Overlap between Hessian of task 1 and gradient at each task')\n",
    "    N = 128\n",
    "    for regime,c in zip(['sp','ntk','mup'],[0,1,2]):\n",
    "\n",
    "        over_rec = []\n",
    "\n",
    "        with open(f'/Users/alessandrobreccia/Desktop/THESIS/data/overlap{N}_{regime}.txt', 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            lst = list(map(float, line.strip(',').split()))\n",
    "            over_rec.append(lst)\n",
    "                \n",
    "        a = np.array(over_rec).T\n",
    "\n",
    "        sh1 = a[0:T]\n",
    "        sh2 = a[T:2*T]\n",
    "        sh3 = a[2*T:3*T]\n",
    "        sh4 = a[3*T:4*T]\n",
    "\n",
    "        for i,sh in enumerate([sh1,sh2,sh3,sh4]):\n",
    "            plt.axvline(i*T, color='r', linestyle='dotted')\n",
    "            for j,row in enumerate(sh.T):\n",
    "                plt.plot(np.arange(i*T,(i+1)*T,1), row, color=colors[c], label= regime)\n",
    "\n",
    "    plt.ylabel('Sharpness')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "OVERLAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NORM():\n",
    "    T = epochs*len(X)//batch\n",
    "    colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'gray', 'orange', 'purple']\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.title(f'Weigths norm of the models')\n",
    "    N = 128\n",
    "    for regime,c in zip(['sp','ntk','mup'],[0,1,2]):\n",
    "\n",
    "        norm_rec = []\n",
    "\n",
    "        with open(f'/Users/alessandrobreccia/Desktop/THESIS/data/norm{N}_{regime}.txt', 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            lst = list(map(float, line.strip(',').split()))\n",
    "            norm_rec.append(lst)\n",
    "                \n",
    "        a = np.array(norm_rec).T\n",
    "\n",
    "        sh1 = a[0:T]\n",
    "        sh2 = a[T:2*T]\n",
    "        sh3 = a[2*T:3*T]\n",
    "        sh4 = a[3*T:4*T]\n",
    "\n",
    "        for i,sh in enumerate([sh1,sh2,sh3,sh4]):\n",
    "            plt.axvline(i*T, color='r', linestyle='dotted')\n",
    "            for j,row in enumerate(sh.T):\n",
    "                plt.plot(np.arange(i*T,(i+1)*T,1), row, color=colors[c], label=regime)\n",
    "\n",
    "    plt.ylabel('Sharpness')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "NORM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(res1)\n",
    "b = np.array(res2)\n",
    "\n",
    "plt.plot(a)\n",
    "plt.axvline(epochs)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(b)\n",
    "plt.axvline(epochs)\n",
    "plt.show()\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from neural_tangents import stax\n",
    "import scipy as sc\n",
    "\n",
    "init_fn, apply_fn, kernel_fn = stax.serial(\n",
    "    stax.Dense(128), stax.Relu(),\n",
    "    stax.Dense(10),\n",
    ")\n",
    "\n",
    "key1, key2 = random.split(random.PRNGKey(1))\n",
    "\n",
    "_, params = init_fn(key1, input_shape=X.shape)\n",
    "\n",
    "ntk22 = kernel_fn(jnp.array(tasks[1].cpu()),jnp.array(tasks[1].cpu()), 'ntk')\n",
    "ntk12 = kernel_fn(jnp.array(tasks[0].cpu()),jnp.array(tasks[1].cpu()), 'ntk')\n",
    "inv_ntk22 = sc.linalg.inv(ntk22)\n",
    "\n",
    "g = mlp.gamma\n",
    "\n",
    "def delta1(t,delta10 , delta20):\n",
    "    term_time = (np.eye(len(X)) - sc.linalg.expm(-np.array(ntk22)*t/g))\n",
    "    kerns_term = ntk12 @ inv_ntk22\n",
    "    d = np.array(delta10) - np.array(delta20) @ kerns_term @ term_time\n",
    "    return d\n",
    "\n",
    "p = []\n",
    "for t in np.linspace(0,1,200):\n",
    "    p.append(delta1(t,a[200],b[200]))\n",
    "\n",
    "x = np.array(p)\n",
    "plt.plot(np.mean(x, axis = 1))\n",
    "plt.plot(np.mean(a[200:], axis= 1) )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
